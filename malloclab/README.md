# CS:APP Malloc Lab

## 简介

这个lab要求我们实现c语言中的`molloc`和`free`，以管理内存。

我们需要设计一个高效的管理字节数组内容的数据结构，其实质在进程运行时就是进程的堆空间。

1. 对于空闲块和分配块，应该使用何种基础数据结构？
2. 如何分配块？如何释放块？
3. 如何合并相邻的空闲块？该在什么时候合并？

需要修改的文件是`mm.c`

```sh
# build
make clean && make
# test
make && ./mdriver -t ../traces/ -v
# debug
make && gdb --args ./mdriver -t ../traces/ -v
```

## 设计

### 前置知识

#### 分配器

显示分配器要在以下要求下工作

- 处理任意请求序列
- 立即响应请求
- 只使用堆
- 对齐块
- 不修改已分配的块

有以下目标：

1. 最大化吞吐率（每个单位时间里完成的请求数）
2. 最大化内存利用率

二者是相互牵制的，需要处理好时间和空间之间的平衡。

#### 碎片

- **内部碎片**是在一个已分配块比有效载荷大时发生的。
- **外部碎片**是当空闲内存合计起来足够满足一个分配请求，但是没有一个单独的空闲块足够大可以来处理这个请求时发生的。

### 隐式空闲链表 (Implicit Free List)

把所有块连接起来，每次分配时从头到尾扫描合适的空闲块，我在实验的第一部分实现了这个做法

### 显式空闲链表 (Explicit Free List)

它是 Implicit Free List 的进一步优化，在所有空闲块中记录两个指针，分别指向前一个空闲块和后一个空闲块，相当于在链表中又嵌套了一个链表，这样分配的时候就不需要遍历已分配块了，将分配时间从块总数的线性时间缩短为空闲块数量的线性时间。可以配合红黑树实现一个高效的分配系统。

### 分离的空闲链表 (Segregated Free List)

维护多个空闲链表，每个链表中的块有大致相等的大小，分配器维护着一个空闲链表数组，每个大小类一个空闲链表，当需要分配块时只需要在对应的空闲链表中搜索就好了，书上描述了三种分离存储的方法：

#### 简单分离存储 (Simple Segregated Storage)

这是一种类似桶排序的做法。空间换时间。

从不合并与分离，每个块的大小就是大小类中最大元素的大小。例如大小类为 {17~32}，则需要分配块的大小在这个区间时均在此对应链表进行分配，并且都是分配大小为 32 的块。

这样做分配和释放都是O(1)的，但空间利用率较低。

#### 分离适配 (Segregated Fit)

这是一种粒度更粗的桶排序，可以搭配一些统计学的应用。

每个大小类的空闲链表包含大小不同的块，分配完一个块后，将这个块进行分割，并根据剩下的块的大小将其插入到适当大小类的空闲链表中。这个做法平衡了搜索时间与空间利用率，GNU C 标准库的 malloc 包就是采用的这种方法。我在实验的第二部分实现了这个做法。

对分离空闲链表的简单的首次适配搜索，其内存利用率近似于对整个堆的最佳适配搜索的内存利用率。

#### 伙伴系统 (buddy system)

伙伴系统是分离适配的一种特例，每个大小类都是2的幂，请求时向上舍入到最近的2的幂类。

它可以实现快速搜索和快速合并。缺点是可能导致内部碎片显著增多。

只用于某些特定应用的工作负载。

### Implicit Free List 实现

CSAPP 详细讲述了基于隐式空闲链表，使用立即边界合并方式的实现，我们回顾一下。

要想分数尽可能高，我们需要使吞吐率尽可能高，空间利用率尽可能小，如果从不重复利用任何块，函数的吞吐率会非常好，而空间利用率会很差；而若进行空闲块的分割合并等操作又会影响吞吐率，因而，就要设计合适的数据结构与算法来平衡两者的关系。

空闲块组织：利用隐式空闲链表记录空闲块
放置策略：如何选择合适的空闲块分配？
首次适配：从头开始搜索空闲链表，选择第一个合适的空闲块
下一次适配：从上一次查询结束的地方开始搜索选择第一个合适的空闲块
最佳适配：搜索能放下请求大小的最小空闲块
分割：在将一个新分配的块放置到某个空闲块后，剩余的部分要进行处理
合并：释放某个块后，要让它与相邻的空闲块合并
无论用什么策略，放置的时间复杂度都为 O(n)，性能很差

## reference

1. [CSAPP: MallocLab多个链表版本的实现与推荐](https://zhuanlan.zhihu.com/p/457373110)
2. [CSAPP | Lab8-Malloc Lab 深入解析](https://zhuanlan.zhihu.com/p/496366818)